{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ‹ï¸ Forma AI - Comprehensive Fitness Training\n",
        "\n",
        "**IMPORTANT: Run cells ONE BY ONE, not \"Run All\"!**\n",
        "\n",
        "This trains Forma AI - a bilingual fitness assistant with:\n",
        "- Egyptian Arabic (Ø§Ù„Ø¹Ø§Ù…ÙŠØ© Ø§Ù„Ù…ØµØ±ÙŠØ©)\n",
        "- Franco Arabic (3aml eh, ezayak)\n",
        "- English\n",
        "\n",
        "**Dataset:** 9,500+ samples including:\n",
        "- Egyptian foods, fast food, supermarkets\n",
        "- Workouts, calisthenics, yoga, flexibility\n",
        "- Boxing, Muay Thai, BJJ, Kickboxing\n",
        "- WHO guidelines, research-based advice\n",
        "- Forma app integration & identity\n",
        "- Equipment adaptations (gym/home)\n",
        "\n",
        "**GPU:** L4 (recommended) - 3x faster than T4\n",
        "**Time:** 3-4 hours for excellent quality\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Check GPU (L4 recommended)"
      ],
      "metadata": {
        "id": "step1_header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check_gpu"
      },
      "outputs": [],
      "source": [
        "# Check GPU - L4 is 3x faster than T4\n",
        "!nvidia-smi --query-gpu=name,memory.total --format=csv\n",
        "\n",
        "import torch\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"âŒ ERROR: No GPU!\")\n",
        "    print(\"Go to Runtime > Change runtime type > L4 GPU\")\n",
        "else:\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    vram = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"âœ… GPU: {gpu_name}\")\n",
        "    print(f\"âœ… VRAM: {vram:.1f} GB\")\n",
        "    \n",
        "    if \"L4\" in gpu_name:\n",
        "        print(\"ðŸš€ L4 detected! Fastest option for training.\")\n",
        "    elif \"A100\" in gpu_name:\n",
        "        print(\"ðŸš€ A100 detected! Premium GPU.\")\n",
        "    elif \"T4\" in gpu_name:\n",
        "        print(\"âš ï¸ T4 detected. Works but L4 is 3x faster.\")\n",
        "    \n",
        "    print(\"\\nâœ… Ready to train!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Install Dependencies\n",
        "\n",
        "**âš ï¸ After this cell, RESTART the session!**\n",
        "- Go to Runtime > Restart session\n",
        "- Then continue from Step 3"
      ],
      "metadata": {
        "id": "step2_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install packages\n",
        "!pip install -q transformers==4.46.0 datasets accelerate peft trl bitsandbytes\n",
        "!pip install -q sentencepiece protobuf\n",
        "\n",
        "print(\"\")\n",
        "print(\"=\" * 60)\n",
        "print(\"âœ… INSTALLATION COMPLETE!\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\")\n",
        "print(\"âš ï¸  NOW: Go to Runtime > Restart session\")\n",
        "print(\"âš ï¸  THEN: Continue from Step 3 (skip this cell)\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "install_deps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Download Training Data"
      ],
      "metadata": {
        "id": "step3_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import urllib.request\n",
        "\n",
        "# Download comprehensive training data\n",
        "DATA_URL = \"https://raw.githubusercontent.com/Abdellatifemara/Forma/main/apps/api/training-data/train_comprehensive.json\"\n",
        "\n",
        "print(\"ðŸ“¥ Downloading Forma AI training data...\")\n",
        "print(\"\")\n",
        "print(\"Dataset includes:\")\n",
        "print(\"  âœ… Egyptian foods, fast food, supermarkets\")\n",
        "print(\"  âœ… Workouts, calisthenics, yoga, flexibility\")\n",
        "print(\"  âœ… Boxing, Muay Thai, BJJ, Kickboxing (extensive)\")\n",
        "print(\"  âœ… WHO guidelines, Olympic training, research\")\n",
        "print(\"  âœ… Forma app identity & features\")\n",
        "print(\"  âœ… Equipment adaptations (gym/home)\")\n",
        "print(\"  âœ… Exercise FAQs & instructions\")\n",
        "print(\"  âœ… Goal-based recommendations\")\n",
        "print(\"  âœ… Egyptian mindset (assertive, motivating)\")\n",
        "print(\"\")\n",
        "\n",
        "urllib.request.urlretrieve(DATA_URL, \"train_data.json\")\n",
        "\n",
        "with open(\"train_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "print(f\"âœ… Loaded {len(data):,} training samples!\")\n",
        "\n",
        "# Show category breakdown\n",
        "categories = {}\n",
        "for item in data:\n",
        "    cat = item.get(\"category\", \"fitness\")\n",
        "    categories[cat] = categories.get(cat, 0) + 1\n",
        "\n",
        "print(\"\\nTop categories:\")\n",
        "for cat, count in sorted(categories.items(), key=lambda x: -x[1])[:15]:\n",
        "    print(f\"  {cat}: {count}\")"
      ],
      "metadata": {
        "id": "download_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Load Model"
      ],
      "metadata": {
        "id": "step4_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "# Qwen2.5-3B - excellent Arabic support\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "\n",
        "print(f\"ðŸ“¥ Loading {MODEL_NAME}...\")\n",
        "print(\"This takes 2-3 minutes...\")\n",
        "\n",
        "# 4-bit quantization for memory efficiency\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# Prepare for training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Add LoRA adapters - higher rank for better quality\n",
        "lora_config = LoraConfig(\n",
        "    r=64,              # Higher rank for better learning\n",
        "    lora_alpha=64,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total = sum(p.numel() for p in model.parameters())\n",
        "print(f\"\")\n",
        "print(f\"âœ… Model loaded!\")\n",
        "print(f\"âœ… Trainable parameters: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)\")"
      ],
      "metadata": {
        "id": "load_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Prepare Dataset"
      ],
      "metadata": {
        "id": "step5_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Forma AI System Prompt - This defines who Forma is\n",
        "SYSTEM_PROMPT = \"\"\"You are Forma AI - a bilingual fitness assistant designed for Egypt.\n",
        "\n",
        "WHO YOU ARE:\n",
        "- Your name is Forma (ÙÙˆØ±Ù…Ø§)\n",
        "- You are the AI inside the Forma fitness app\n",
        "- You have an Egyptian mindset - assertive, motivating, direct\n",
        "- You understand: Egyptian Arabic, Franco Arabic, and English\n",
        "\n",
        "WHAT YOU KNOW:\n",
        "- Exercise technique and programming (all types)\n",
        "- Combat sports: Boxing, Muay Thai, BJJ, Kickboxing\n",
        "- Nutrition with Egyptian foods, fast food, restaurants\n",
        "- WHO guidelines and research-based recommendations\n",
        "- Home workouts, resistance bands, bodyweight training\n",
        "- Yoga, flexibility, and mixed training\n",
        "- Supplements (evidence-based only)\n",
        "\n",
        "HOW YOU HELP:\n",
        "- You know the Forma app features and can guide users\n",
        "- You adapt to user's equipment (gym vs home)\n",
        "- You create workout and meal plans\n",
        "- You track progress and give personalized advice\n",
        "- You recommend doctors for injuries/medical issues\n",
        "\n",
        "YOUR STYLE:\n",
        "- Be direct and motivating\n",
        "- Give practical, actionable advice\n",
        "- Don't overcomplicate things\n",
        "- Push users to be better, but be supportive\"\"\"\n",
        "\n",
        "# Clean and prepare data\n",
        "clean_data = []\n",
        "for item in data:\n",
        "    instruction = item.get('instruction', item.get('input', ''))\n",
        "    output = item.get('output', item.get('response', ''))\n",
        "    \n",
        "    # Skip if either is empty/None\n",
        "    if instruction and output and len(str(instruction)) > 0 and len(str(output)) > 0:\n",
        "        clean_data.append({\n",
        "            'instruction': str(instruction),\n",
        "            'output': str(output),\n",
        "            'category': item.get('category', 'fitness')\n",
        "        })\n",
        "\n",
        "print(f\"Original samples: {len(data):,}\")\n",
        "print(f\"Clean samples: {len(clean_data):,}\")\n",
        "print(f\"Removed {len(data) - len(clean_data)} bad samples\")\n",
        "\n",
        "def format_chat(item):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": item['instruction']},\n",
        "        {\"role\": \"assistant\", \"content\": item['output']},\n",
        "    ]\n",
        "    \n",
        "    text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "    return {\"text\": text}\n",
        "\n",
        "dataset = Dataset.from_list(clean_data)\n",
        "dataset = dataset.map(format_chat)\n",
        "\n",
        "print(f\"\")\n",
        "print(f\"âœ… Dataset ready: {len(dataset):,} samples\")\n",
        "print(f\"\")\n",
        "print(\"Example:\")\n",
        "print(dataset[100]['text'][:500] + \"...\")"
      ],
      "metadata": {
        "id": "prepare_dataset"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: TRAIN! ðŸš€\n",
        "\n",
        "**Watch the loss value decrease:**\n",
        "- Starting: ~2.5\n",
        "- After 2000 steps: ~0.8\n",
        "- After 5000 steps: ~0.4\n",
        "- After 10000 steps: ~0.25 (excellent!)\n",
        "\n",
        "**Training Time (L4 GPU):**\n",
        "- 10,000 steps: ~3-4 hours\n",
        "- Model saves every 2000 steps to Google Drive\n",
        "- You can stop and resume if needed"
      ],
      "metadata": {
        "id": "step6_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "from google.colab import drive\n",
        "import time\n",
        "\n",
        "# Mount Google Drive FIRST for checkpoints\n",
        "drive.mount('/content/drive')\n",
        "SAVE_PATH = \"/content/drive/MyDrive/Forma-AI-Model\"\n",
        "!mkdir -p {SAVE_PATH}\n",
        "\n",
        "# =====================================================\n",
        "# TRAINING SETTINGS - Optimized for L4 GPU\n",
        "# =====================================================\n",
        "MAX_STEPS = 10000     # High quality - ~3-4 hours on L4\n",
        "BATCH_SIZE = 4        # L4 can handle bigger batches\n",
        "GRAD_ACCUM = 4        # Effective batch = 16\n",
        "LEARNING_RATE = 1e-4  # Slower learning for better quality\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"ðŸš€ TRAINING STARTED\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Steps: {MAX_STEPS:,}\")\n",
        "print(f\"Effective batch size: {BATCH_SIZE * GRAD_ACCUM}\")\n",
        "print(f\"Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"Estimated time on L4: ~3-4 hours\")\n",
        "print(\"\")\n",
        "print(\"ðŸ“Š Watch the 'loss' value decrease over time!\")\n",
        "print(\"âœ… Good final loss: < 0.3\")\n",
        "print(\"ðŸ’¾ Saves to Google Drive every 2000 steps\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    args=SFTConfig(\n",
        "        output_dir=SAVE_PATH,  # Save directly to Drive\n",
        "        per_device_train_batch_size=BATCH_SIZE,\n",
        "        gradient_accumulation_steps=GRAD_ACCUM,\n",
        "        warmup_steps=200,\n",
        "        max_steps=MAX_STEPS,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        fp16=True,\n",
        "        logging_steps=100,\n",
        "        save_steps=2000,       # Save checkpoint every 2000 steps\n",
        "        save_total_limit=3,    # Keep last 3 checkpoints\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        seed=42,\n",
        "        report_to=\"none\",\n",
        "        dataset_text_field=\"text\",\n",
        "        max_seq_length=2048,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Train!\n",
        "stats = trainer.train()\n",
        "\n",
        "duration = (time.time() - start_time) / 60\n",
        "print(\"\")\n",
        "print(\"=\" * 60)\n",
        "print(\"âœ… TRAINING COMPLETE!\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Time: {duration:.1f} minutes\")\n",
        "print(f\"Final loss: {stats.training_loss:.4f}\")\n",
        "print(\"\")\n",
        "if stats.training_loss < 0.3:\n",
        "    print(\"ðŸ† EXCELLENT! Loss is very low. Model learned exceptionally well.\")\n",
        "elif stats.training_loss < 0.5:\n",
        "    print(\"âœ… GOOD! Loss is low. Model learned well.\")\n",
        "elif stats.training_loss < 1.0:\n",
        "    print(\"âš ï¸ OKAY. Model learned the basics. Consider more training.\")\n",
        "else:\n",
        "    print(\"âŒ HIGH. Something might be wrong. Check the logs.\")"
      ],
      "metadata": {
        "id": "train_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Save Final Model"
      ],
      "metadata": {
        "id": "step7_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the final model\n",
        "print(\"ðŸ’¾ Saving final model to Google Drive...\")\n",
        "\n",
        "FINAL_PATH = \"/content/drive/MyDrive/Forma-AI-Model/final\"\n",
        "!mkdir -p {FINAL_PATH}\n",
        "\n",
        "model.save_pretrained(FINAL_PATH)\n",
        "tokenizer.save_pretrained(FINAL_PATH)\n",
        "\n",
        "print(f\"âœ… Model saved to: {FINAL_PATH}\")"
      ],
      "metadata": {
        "id": "save_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Test the Model"
      ],
      "metadata": {
        "id": "step8_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the trained model\n",
        "model.eval()\n",
        "\n",
        "test_prompts = [\n",
        "    # English\n",
        "    \"Who are you?\",\n",
        "    \"How many calories in Koshari?\",\n",
        "    \"How do I do a proper bench press?\",\n",
        "    \"What does WHO recommend for exercise?\",\n",
        "    \"I want to learn boxing basics\",\n",
        "    # Arabic\n",
        "    \"Ø§Ù†Øª Ù…ÙŠÙ†ØŸ\",\n",
        "    \"ÙƒØ§Ù… Ø³Ø¹Ø±Ø© ÙÙŠ Ø§Ù„ÙÙˆÙ„ØŸ\",\n",
        "    \"Ø§Ø²Ø§ÙŠ Ø§Ø¨Ù†ÙŠ Ø¹Ø¶Ù„ØŸ\",\n",
        "    # Franco\n",
        "    \"kam calorie fel koshari?\",\n",
        "    \"3ayz atamren fel beit men 8er gym\",\n",
        "]\n",
        "\n",
        "print(\"ðŸ§ª Testing model...\\n\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        "    \n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(\"cuda\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs,\n",
        "            max_new_tokens=200,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "        )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # Extract assistant response\n",
        "    if \"assistant\" in response.lower():\n",
        "        response = response.split(\"assistant\")[-1].strip()\n",
        "    \n",
        "    print(f\"Q: {prompt}\")\n",
        "    print(f\"A: {response[:400]}\")\n",
        "    print(\"-\" * 60)"
      ],
      "metadata": {
        "id": "test_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Export to GGUF (for Ollama)\n",
        "\n",
        "This converts the model to GGUF format for use with Ollama on your computer/server."
      ],
      "metadata": {
        "id": "step9_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge LoRA weights with base model\n",
        "print(\"ðŸ”„ Merging LoRA weights with base model...\")\n",
        "print(\"This takes 5-10 minutes...\")\n",
        "\n",
        "merged_model = model.merge_and_unload()\n",
        "\n",
        "MERGED_PATH = \"/content/forma-merged\"\n",
        "merged_model.save_pretrained(MERGED_PATH)\n",
        "tokenizer.save_pretrained(MERGED_PATH)\n",
        "\n",
        "print(\"âœ… Model merged and saved!\")"
      ],
      "metadata": {
        "id": "merge_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to GGUF format\n",
        "print(\"ðŸ”„ Converting to GGUF format...\")\n",
        "print(\"This takes 10-15 minutes...\")\n",
        "\n",
        "# Clone llama.cpp\n",
        "!git clone --depth 1 https://github.com/ggerganov/llama.cpp\n",
        "!pip install -q -r llama.cpp/requirements.txt\n",
        "\n",
        "# Convert to GGUF (Q4_K_M is good balance of size/quality)\n",
        "!python llama.cpp/convert_hf_to_gguf.py /content/forma-merged --outfile /content/forma-fitness.gguf --outtype q4_k_m\n",
        "\n",
        "# Copy to Google Drive\n",
        "!cp /content/forma-fitness.gguf \"/content/drive/MyDrive/Forma-AI-Model/forma-fitness.gguf\"\n",
        "\n",
        "# Get file size\n",
        "import os\n",
        "size_gb = os.path.getsize(\"/content/forma-fitness.gguf\") / (1024**3)\n",
        "\n",
        "print(\"\")\n",
        "print(\"=\" * 60)\n",
        "print(\"âœ… GGUF CONVERSION COMPLETE!\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\")\n",
        "print(f\"ðŸ“¦ File size: {size_gb:.2f} GB\")\n",
        "print(\"ðŸ“ Saved to: Google Drive > Forma-AI-Model > forma-fitness.gguf\")\n",
        "print(\"\")\n",
        "print(\"To use with Ollama:\")\n",
        "print(\"1. Download forma-fitness.gguf from Google Drive\")\n",
        "print(\"2. Create Modelfile with:\")\n",
        "print(\"   FROM ./forma-fitness.gguf\")\n",
        "print(\"   SYSTEM \\\"You are Forma AI...\\\"\")\n",
        "print(\"3. Run: ollama create forma-fitness -f Modelfile\")\n",
        "print(\"4. Test: ollama run forma-fitness\")"
      ],
      "metadata": {
        "id": "convert_gguf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸŽ‰ Done!\n",
        "\n",
        "Your Forma AI model is now:\n",
        "1. **Saved in Google Drive** as LoRA adapter + tokenizer\n",
        "2. **Exported as GGUF** for Ollama deployment\n",
        "\n",
        "### What's Next?\n",
        "1. Download the GGUF file from Google Drive\n",
        "2. Set up Ollama on your server\n",
        "3. Integrate with your Forma app API\n",
        "\n",
        "### Model Capabilities:\n",
        "- âœ… Egyptian Arabic understanding\n",
        "- âœ… Franco Arabic support\n",
        "- âœ… English fluency\n",
        "- âœ… Egyptian foods & nutrition\n",
        "- âœ… Workouts for gym and home\n",
        "- âœ… Combat sports (Boxing, Muay Thai, BJJ, Kickboxing)\n",
        "- âœ… WHO guidelines & research-based advice\n",
        "- âœ… Forma app integration\n",
        "- âœ… Assertive, motivating Egyptian style"
      ],
      "metadata": {
        "id": "done"
      }
    }
  ]
}
