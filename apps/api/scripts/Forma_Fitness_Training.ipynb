{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üèãÔ∏è Forma Fitness - AI Chatbot Training\n",
        "\n",
        "This notebook trains a bilingual (Arabic/English) fitness chatbot for the Forma app.\n",
        "\n",
        "**What it does:**\n",
        "- Trains Qwen2.5-3B-Instruct (supports Arabic!)\n",
        "- Uses your 8,140 fitness Q&A samples\n",
        "- Exports to GGUF format for Ollama\n",
        "- Saves to Google Drive\n",
        "\n",
        "**Time:** ~30-45 minutes on T4, ~15-20 minutes on A100\n",
        "\n",
        "---\n",
        "## Step 1: Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check what GPU we got\n",
        "!nvidia-smi --query-gpu=name,memory.total --format=csv\n",
        "\n",
        "import torch\n",
        "print(f\"\\nPyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Install Unsloth for 2-3x faster training\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps trl peft accelerate bitsandbytes xformers\n",
        "!pip install datasets huggingface_hub\n",
        "\n",
        "print(\"‚úÖ Dependencies installed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Download Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import urllib.request\n",
        "\n",
        "# Download from your GitHub repo\n",
        "DATA_URL = \"https://raw.githubusercontent.com/Abdellatifemara/Forma/main/apps/api/training-data/train_balanced.json\"\n",
        "\n",
        "print(\"Downloading training data...\")\n",
        "urllib.request.urlretrieve(DATA_URL, \"train_data.json\")\n",
        "\n",
        "with open(\"train_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(data)} training samples!\")\n",
        "print(f\"\\nSample categories:\")\n",
        "categories = {}\n",
        "for item in data:\n",
        "    cat = item.get('category', 'unknown')\n",
        "    categories[cat] = categories.get(cat, 0) + 1\n",
        "for cat, count in sorted(categories.items(), key=lambda x: -x[1])[:10]:\n",
        "    print(f\"  {cat}: {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Load Model with Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Model config\n",
        "MODEL_NAME = \"unsloth/Qwen2.5-3B-Instruct-bnb-4bit\"  # Supports Arabic!\n",
        "MAX_SEQ_LENGTH = 2048\n",
        "LORA_R = 32  # Higher = more capacity\n",
        "\n",
        "print(f\"Loading {MODEL_NAME}...\")\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=MODEL_NAME,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    dtype=None,  # Auto-detect\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "# Add LoRA adapters\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=LORA_R,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                   \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=LORA_R,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Model loaded with LoRA!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# System prompt for Forma\n",
        "SYSTEM_PROMPT = \"\"\"You are Forma AI, a bilingual fitness assistant for Egypt.\n",
        "\n",
        "You provide accurate, science-based advice on:\n",
        "- Exercise technique and programming\n",
        "- Nutrition and meal planning\n",
        "- Supplements (evidence-based only)\n",
        "\n",
        "You understand:\n",
        "- Arabic (Egyptian dialect)\n",
        "- Franco Arabic (3aml eh, ezayak)\n",
        "- English\n",
        "\n",
        "You know Egyptian foods, gyms, and local fitness culture.\n",
        "Always prioritize user safety. Recommend doctors for injuries/medical issues.\"\"\"\n",
        "\n",
        "def format_chat(item):\n",
        "    \"\"\"Convert to ChatML format\"\"\"\n",
        "    instruction = item.get('instruction', item.get('input', ''))\n",
        "    output = item.get('output', item.get('response', ''))\n",
        "    \n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": instruction},\n",
        "        {\"role\": \"assistant\", \"content\": output},\n",
        "    ]\n",
        "    \n",
        "    text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "    return {\"text\": text}\n",
        "\n",
        "# Create dataset\n",
        "dataset = Dataset.from_list(data)\n",
        "dataset = dataset.map(format_chat)\n",
        "\n",
        "print(f\"‚úÖ Dataset ready: {len(dataset)} samples\")\n",
        "print(f\"\\nExample:\")\n",
        "print(dataset[0]['text'][:500] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Train! üöÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# Training config\n",
        "MAX_STEPS = 2000  # Increase for better quality\n",
        "BATCH_SIZE = 2\n",
        "GRAD_ACCUM = 4  # Effective batch = 8\n",
        "LEARNING_RATE = 2e-4\n",
        "\n",
        "print(f\"Starting training...\")\n",
        "print(f\"  Steps: {MAX_STEPS}\")\n",
        "print(f\"  Batch size: {BATCH_SIZE} x {GRAD_ACCUM} = {BATCH_SIZE * GRAD_ACCUM}\")\n",
        "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    dataset_num_proc=2,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=BATCH_SIZE,\n",
        "        gradient_accumulation_steps=GRAD_ACCUM,\n",
        "        warmup_steps=50,\n",
        "        max_steps=MAX_STEPS,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=25,\n",
        "        save_steps=500,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        seed=42,\n",
        "        output_dir=\"forma-fitness-model\",\n",
        "        report_to=\"none\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Train!\n",
        "stats = trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\"‚úÖ Training complete!\")\n",
        "print(f\"   Final loss: {stats.training_loss:.4f}\")\n",
        "print(f\"   Time: {stats.metrics['train_runtime']/60:.1f} minutes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "test_prompts = [\n",
        "    \"How many sets should I do for muscle growth?\",\n",
        "    \"What Egyptian foods are high in protein?\",\n",
        "    \"I have a shoulder injury, what exercises can I do?\",\n",
        "    \"What is Forma?\",\n",
        "]\n",
        "\n",
        "print(\"Testing model...\\n\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        "    \n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(\"cuda\")\n",
        "    \n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs,\n",
        "        max_new_tokens=150,\n",
        "        use_cache=True,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "    )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "    if \"<|im_start|>assistant\" in response:\n",
        "        response = response.split(\"<|im_start|>assistant\")[-1]\n",
        "        response = response.split(\"<|im_end|>\")[0].strip()\n",
        "    \n",
        "    print(f\"Q: {prompt}\")\n",
        "    print(f\"A: {response[:300]}...\")\n",
        "    print(\"-\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Save to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Save LoRA model\n",
        "SAVE_PATH = \"/content/drive/MyDrive/Forma-AI-Model\"\n",
        "!mkdir -p {SAVE_PATH}\n",
        "\n",
        "print(\"Saving LoRA model...\")\n",
        "model.save_pretrained(SAVE_PATH)\n",
        "tokenizer.save_pretrained(SAVE_PATH)\n",
        "\n",
        "print(f\"‚úÖ Model saved to: {SAVE_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Export to GGUF (for Ollama)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to GGUF format for Ollama/llama.cpp\n",
        "GGUF_PATH = f\"{SAVE_PATH}/gguf\"\n",
        "!mkdir -p {GGUF_PATH}\n",
        "\n",
        "print(\"Converting to GGUF (q4_k_m quantization)...\")\n",
        "print(\"This may take 5-10 minutes...\\n\")\n",
        "\n",
        "model.save_pretrained_gguf(\n",
        "    GGUF_PATH,\n",
        "    tokenizer,\n",
        "    quantization_method=\"q4_k_m\"  # Good balance of speed/quality\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ GGUF saved to: {GGUF_PATH}\")\n",
        "print(\"\\nTo use with Ollama:\")\n",
        "print(\"1. Download the .gguf file from Google Drive\")\n",
        "print(\"2. Create a Modelfile with: FROM ./your-model.gguf\")\n",
        "print(\"3. Run: ollama create forma-fitness -f Modelfile\")\n",
        "print(\"4. Test: ollama run forma-fitness\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéâ Done!\n",
        "\n",
        "Your model is now:\n",
        "1. **Saved as LoRA** in Google Drive (`Forma-AI-Model/`)\n",
        "2. **Exported as GGUF** for Ollama (`Forma-AI-Model/gguf/`)\n",
        "\n",
        "### Next Steps:\n",
        "1. Download the `.gguf` file from Google Drive\n",
        "2. Install [Ollama](https://ollama.ai)\n",
        "3. Create model: `ollama create forma-fitness -f Modelfile`\n",
        "4. Integrate with your Forma app's `/api/chat-offline` endpoint"
      ]
    }
  ]
}
